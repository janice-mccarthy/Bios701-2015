
\documentclass[12pt]{article} % Use A4 paper with a 12pt font size - different paper sizes will require manual recalculation of page margins and border positions

% Generated with LaTeXDraw 2.0.8
% Mon Jun 17 19:00:40 EDT 2013
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[left=1.3cm,right=4.6cm,top=1.8cm,bottom=4.0cm,marginparwidth=3.4cm]{geometry} % Adjust page margins
\usepackage{amsmath} % Required for equation customization
\usepackage{amssymb} % Required to include mathematical symbols
\usepackage{xcolor} % Required to specify colors by name
\usepackage{amsthm}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds,trees}
\usepackage{wasysym}

\makeatletter
\newcommand{\infi}{\int_{-\infty}^\infty}
\newcommand{\mytag}[2]{%
  \text{#1}%
  \@bsphack
  \protected@write\@auxout{}%
         {\string\newlabel{#2}{{#1}{\thepage}}}%
  \@esphack
}
\makeatother

\setlength{\parindent}{0cm} % Remove paragraph indentation
\newcommand{\tab}{\hspace*{2em}} % Defines a new command for some horizontal space
%\newcommand{\choose}[2]{\left(\begin{matrix}
%{#1}\\{#2}
%\end{matrix}\right)}
\date{}
\title{Introduction to Probability Theory - Lecture 21}
%----------------------------------------------------------------------------------------
\newcommand{\pdf}[2]{\left\{\begin{matrix}
{#1} & {#2}\\\\\\0&\textrm{otherwise}
\end{matrix}\right.}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{exer}{Exercises}
\newtheorem{thm}{Therorem}

\begin{document}
\maketitle

\section{Independence of Continuous Random Variables}
\begin{defn}
Two random variables $X$ and $Y$ are independent $\iff$ 
$$F_{XY}(x,y) = F_x(x)F_Y(y)$$
where $F_X(x)$ and $F_Y(y)$ are the marginal CDFs for $X$ and $Y$. An equivalent statement is that the joint pdf factors into the product of the marginals:
$$f_{XY}(x,y) = f_X(x)f_Y(y)$$
\end{defn} 
Note: The text has an example of the continuous uniform distribution defined on the unit circle in the plane. In this case, the pdf is:
$$f_{XY}(x,y) = \left\{\begin{matrix}\frac1\pi & \textrm{for } x^2+y^2 =1\\0&\textrm{otherwise}\end{matrix}\right.$$
This pdf factors (trivially) into (constant) functions of $x$ and $y$, but those factors are \emph{not} the marginals of $X$ and $Y$.\\\\
However, there is a way to check for independence \emph{without} computing the marginals:
\begin{thm}
$X$ and $Y$ are independent $\iff$
$$f_{XY}(x,y) = g(x)h(y)$$
and \emph{the support set is a rectangle}.
\end{thm}
\begin{example}
Consider random variables $X_1$ and $X_2$ with the following joint pdf:
$$f(x_1,x_2) = \left\{\begin{matrix}
8x_1x_2 & 0<x_1<x_2<1\\\\\\0&\textrm{otherwise}
\end{matrix}\right.$$
This pdf clearly factors into a product of the form $g(x_1)h(x_2)$, but the support set is \emph{triangular}. Thus, $X_1$ and $X_2$ are not independent. Let's compute the marginals to see the dependence another way. The marginal for $X_1$ is
$$f_1(x_1) = \int_{-\infty}^\infty f(x_1,x_2)dx_2$$
Now, we need to integrate over all \emph{possible} values of $x_2$. Because the support is $0<x_1<x_2<1$, we see that $x_2$ is constrained to be between $x_1$ and $1$. Therefore:
 $$f_1(x_1) = \int_{-\infty}^\infty f(x_1,x_2)dx_2 = \int_{x_1}^1 8x_1x_2 dx_2$$
 $$= \left. 8x_1 \frac{x^2_2}2\right\rvert_{x_1}^1 = 4x_1-4x_1^3 \;\;\;\;\; \textrm{for }\;\;\; 0< x_1 <1$$
 We can also compute the marginal for $X_2$:
 $$f_2(x_2) = \int_{-\infty}^\infty f(x_1,x_2)dx_1$$
 The support constrains $x_1$ so that $0<x_1<x_2$, so:
 $$f_2(x_2) = \int_{0}^{x_2} 8x_1x_2dx_1 = \left.8\frac{x_1^2}2 x_2\right\rvert_0^{x_2} = 4x_2^3 \;\;\;\;\textrm{for } \;\;\; 0< x_2< 1$$
 
 Clearly, $f(x_1,x_2) = 8x_1x_2 \neq f_1(x_1)f_2(x_2)$, so we see that $X_1$ and $X_2$ are not independent. \\\\
As these variables are not independent, it makes sense to consider their conditional pdf:
$$f_{X_2|x_1}(x_2) = \frac{f(x_1,x_2)}{f_1(x_1)} = \frac{8x_1x_2}{4x_1(1-x_1^2)} =  \frac{2x_2}{(1-x_1^2)} \;\;\;\;\textrm{for } \;\;\; x_1< x_2< 1$$
\end{example}
\subsection{Covariance and Correlation}
Covariance and correlation are important summaries of joint distributions. They tell us how two random variables vary with respect to one another:
\begin{defn}
The covariance between two random variables $X$ and $Y$ is defined to be:
$$Cov(X,Y) = E\left(\left(X-E(X)\right)\left(Y-E(Y)\right)\right)$$
\end{defn}
Qualitatively, if $X$ and $Y$ increase or decrease together, the covariance will be positive. If $X$ increases as $Y$ decreases, the covariance will be negative.\\\\
Note that we have a formula for the covariance analogous to our variance formula:
$$Var(X) = E(X^2) - \left(E(X)\right)^2$$
and that is:
$$Cov(X,Y) = E(XY)- E(X)E(Y)$$
\begin{thm}
If $X$ and $Y$ are independent, then $Cov(X,Y) = 0$.
\end{thm} 
\begin{proof}
We'll prove this for the case of two continuous random variables. The proof for the discrete case follows pretty much by substituting sums for the integrals. \\\\
Note:
$$E(XY) = \infi\infi xy f(x,y) dx dy = \infi\infi xy f_X(x)f_Y(y)dxdy$$
$$ = \infi xf_X(x)dx\infi y f_Y(y)dy = E(X)E(Y)$$
\end{proof}
Note: THE CONVERSE IS FALSE!!! Two random variables may have zero covariance, but still may not be !\\
\begin{example}
Let $X\sim N(0,1)$ and define $Y=X^2$. Then $Cov(X,Y) = E(XY) = E(X^3)$, which is the third moment of the standard normal, and we know that vanishes. But $Y$ is \emph{clearly} dependent on $X$.
\end{example}
We might ask why our definition of covariance does not detect this kind of relationship. It is because covariance measures \emph{linear} dependence. In the above example, $Y$ and $X$ have no linear dependence (just quadratic).\\\\
\paragraph{Key Properties of Covariance}
\begin{enumerate}
\item $$Cov(X,X) = Var(X)$$
\item $$Cov(X,Y) = Cov(Y,X)$$
\item $$Cov(X,c) = 0$$
\item $$Cov(aX,Y) = aCov(X,Y)$$
\item $$Cov(X+Y,Z) = Cov(X,Z) + Cov(Y,Z)$$
\item $$Var(X+Y) = Var(X)+Var(Y) + 2Cov(X,Y)$$
\end{enumerate}

It is often useful to 'normalize' the covariance in such a way that it is unitless.

\begin{defn}
The correlation between two random variables $X$ and $Y$ is 
$$corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$
\end{defn}
\begin{thm}
For any two random variables $X$ and $Y$, 
$$-1\leq corr(X,Y)\leq 1$$
\end{thm}
This theorem is proved in the text. We will not repeat it here.\\\\
The following is an illustration of correlation.
\begin{example}
Let $X$ and $Y$ be jointly distributed random variables, whose joint distribution is $Unif(C)$, where
$$ C = \left\{(x,y)|0\leq x\leq 10,x-1<y<x+1\right\}$$
The pdf is:
$$f(x,y) = \pdf{{\frac{1}{20}}}{(x,y)\in C}$$
For practice, let's compute the marginal pdfs:
$$f_X(x) = \infi f(x,y) dy = \int_{x-1}^{x+1} \frac1{20} dy = \left. \frac{y}{20}\right\rvert_{x-1}^{x+1} = \frac{x+1-(x-1)}{20} = \frac2{20}=\frac1{10}$$
I.e. $X$ is marginally uniform on $(0,10)$. Now,
$$f_Y(y) = $$
$$corr(X,Y) = $$
\end{example}
\section{Multivariate Normal Distribution}
Usually, the MVN is defined via its pdf. The book uses an alternative (equivalent) definition:
\begin{defn}
A random vector $X=(X_1,...,X_k) \sim MVN(\mu,\Sigma) \iff$ every linear combination of $X_1,...,X_k$ has the normal distribution. I.e., for any $t_1,...,t_k$ such that $t_i$ is a real number for all $i=1,...,k$, 
$$t_1X_1 +...+ t_kX_k$$
is a (one-dimensional) normal random variable.
\end{defn}
Note: Each $X_1,...,X_k$ is marginally normal - BUT - it is entirely possible for a collection of random variables to be marginally normal, with a joint distribution that is non-normal! We will focus on the bivariate normal, but everything we discuss with regard to the BVN extends to the MVN. You are encouraged to study Example 7.5.2 in the text, and read the rest of the section on MVN. The most important considerations are:
\begin{itemize}
\item If ALL linear combinations of $X_1,...,X_k$ are normal, then the random vector $X=(X_1,...,X_k$ is MVN (by our definition).
\item All the marginals of an MVN random vector are normal.
\item Even if all marginals are normal, a random vector is not necessarily MVN.
\end{itemize}
\section{Bivariate Normal}
\begin{defn}
A random vector $X=(X_1,X_2) \sim BVN(\mu_x,\mu_y,\sigma_x^2\sigma_y^2\rho) \iff$ its pdf has the form:
$$f_{XY}(x,y) = \frac{
1}{2\pi\sqrt{1-\rho^2}} \exp\left(\frac{-1}{2(1-\rho^2)}\left(x^2+y^2 -2\rho x y\right)\right)\;\;\;\; \textrm{ for } \;\; (x,y) \in \mathbb{R}^2$$
where $-1\leq \rho\leq 1$ is $corr(x,y)$.
\end{defn}
We present an important property of the BVN, but note that it also extends to the MVN:
\begin{thm}
If $\bf{X}=(X,Y)\sim BVN$ and $corr(x,y)=0$, then $X$ and $Y$ are independent.
\end{thm}
This is a special case where uncorrelated \emph{does} imply independence!
\begin{proof}
If $X,Y$ are BVN, then  
$$f_{XY}(x,y) =  \frac{
1}{2\pi\sqrt{1-\rho^2}} \exp\left(\frac{-1}{2(1-\rho^2)}\left(x^2+y^2 -2\rho x y\right)\right)$$
For $\rho=0:$
$$f_{XY}(x,y) =  \frac{
1}{2\pi} \exp\left(\frac{-1}{2}\left(x^2+y^2\right)\right) = \frac{1}{\sqrt{2\pi}}\exp\left(\frac{-x^2}{2}\right)\frac{1}{\sqrt{2\pi}}\exp\left(\frac{-y^2}{2}\right) = f_X(x)f_Y(y)$$
\end{proof}
\end{document}