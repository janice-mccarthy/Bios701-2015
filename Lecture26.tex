
\documentclass[12pt]{article} % Use A4 paper with a 12pt font size - different paper sizes will require manual recalculation of page margins and border positions

% Generated with LaTeXDraw 2.0.8
% Mon Jun 17 19:00:40 EDT 2013
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}

\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[left=1.3cm,right=4.6cm,top=1.8cm,bottom=4.0cm,marginparwidth=3.4cm]{geometry} % Adjust page margins
\usepackage{amsmath} % Required for equation customization
\usepackage{amssymb} % Required to include mathematical symbols
\usepackage{xcolor} % Required to specify colors by name
\usepackage{amsthm}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{shapes,backgrounds,trees}
\usepackage{wasysym}
\usepackage{scrextend}
\makeatletter
\newcommand{\infi}{\int_{-\infty}^\infty}
\newcommand{\mytag}[2]{%
  \text{#1}%
  \@bsphack
  \protected@write\@auxout{}%
         {\string\newlabel{#2}{{#1}{\thepage}}}%
  \@esphack
}
\makeatother

\setlength{\parindent}{0cm} % Remove paragraph indentation
\newcommand{\tab}{\hspace*{2em}} % Defines a new command for some horizontal space
%\newcommand{\choose}[2]{\left(\begin{matrix}
%{#1}\\{#2}
%\end{matrix}\right)}
\date{}
\title{Introduction to Probability Theory - Lecture 26}
%----------------------------------------------------------------------------------------
\newcommand{\pdf}[2]{\left\{\begin{matrix}
{#1} & {#2}\\\\\\0&\textrm{otherwise}
\end{matrix}\right.}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{exer}{Exercises}
\newtheorem{thm}{Therorem}
%\pgfplotsset{compat=1.12}
\begin{document}
\maketitle

\section{Inequalities and Bounds on Probability}
Often it is desirable to put bounds on the probability of a particular event. We will cover three different inequality theorems: Markov, Chebyshev and Chernoff (named for those who first proved them). 

\subsection{Markov's Inequality}

\begin{thm}
For any random variable $X$ and constant $a>0$
$$P(|X|\geq a) \leq \frac{E(|X|)}{a}$$
\end{thm}
\begin{proof}
Let $Y=\frac{|X|}{a}$. Then
$$P(|X|\geq a) \leq \frac{E(|X|)}{a} \iff P(Y\geq 1) \leq E(Y)$$
so it suffices to show $P(Y\geq 1) \leq E(Y)$. Recall that the indicator variable $I$ is defined so that $I(x)=1$ if $x$ is true and zero otherwise. Note that
$$I(Y\leq 1)\leq Y$$
To see this, note that if $Y<1$, then $I(Y\geq 1) = 0$, and because $Y$ is defined to be $|X|/a$, we know $Y$ is postive, so it must then be greater than  or equal to $I(Y\geq 1)$. On the other hand, if $I(Y\geq 1) = 1$, the indicator variable telss us that $Y\geq 1 = I(Y\geq 1)$.
Now, taking expectations across the inequality:
$$E(I(Y\leq 1))\leq E(Y)$$ 
But:
$$E(I(Y\leq 1)) = I(0)P(I=0)+ I(1)P(I=1) = 0 + P(I=1) = P(Y\geq 1)$$
So 
$$E(I(Y\leq 1))\leq E(Y) \Rightarrow P(Y\geq 1) \leq E(Y)$$ 
\end{proof}
Now, in and of itself, this theorem isn't terribly useful. It bounds probability using expected value, and expected value can be \emph{very} large (even infinite). We already know $P\leq1$, so this really isn't informative. BUT - it is useful in proving some other results! So, on to Chebyshev!

\subsection{Chebyshev's Inequality}
\begin{thm}
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. Then for any $a>0$
$$P((X-\mu)^2\geq a)\leq \frac{\sigma^2}{a^2}$$
\end{thm}
\begin{proof}
Note:
$$P(|X-\mu|\geq a) = P((X-\mu)^2\geq a^2)$$
By Markov's inequality,
$$P((X-\mu)^2\geq a^2) \leq \frac{E((X-\mu)^2)}{a^2}$$
and 
$$E((X-\mu)^2) = \sigma^2$$
by definition, so
$$P((X-\mu)^2\geq a^2) \leq \frac{\sigma^2}{a^2}$$
\end{proof}
We can set $a=c\sigma$ for some $c>0$. This yields a slightly different form of Chebyshev:
$$P(|X-\mu| \geq c\sigma)\leq \frac1{c^2}$$
In other words, we can put bounds on the probability a random variable takes values that are more than $c$ standard deviations from the mean. This is important enough to warrant an example:\\\\
\begin{example}
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. What is the probability that $X$ takes values that are more than 2 standard deviations from its mean?\\\\
By Chebyshev:
$$P(|X-\mu|\geq 2\sigma) \leq \frac14$$.
This means that the probability that $X$ takes values that are further than 2 standard deviations from the mean is $25\%$. Now, that's a better bound than Markov, but we know that for the normal distribution $P(|X-\mu|>2\sigma) = 0.05$. Chebyshev gives us \emph{an} upper bound, but it may not be close to the best.
\end{example}
The next inequality theorem gives us a slightly better bound.

\subsection{Chernoff's Inequality}
\begin{thm}
For any random variable $X$ and constants $a>0,t>0$
$$P(X>a)\leq \frac{E(e^{tx})}{e^{ta}}$$
\end{thm}
\begin{proof}
Because $e^{tx}$ is a 1-1, increasing function, we can write:
$$P(X>a) = P(e^{tx} \geq e^{ta})$$
By Markov:
$$ P(e^{tx} \geq e^{ta}) \leq \frac{E(e^{tx})}{e^{ta}}$$
\end{proof}
The numerator on the right hand side of the inequality is the MGF of $X$. Therefore, if the MGF exists, we can minimize the right hand side (i.e. find the value of $t$ that minimizes the bound). We will see this in the following example:\\\\
\begin{example}
Let $Z\sim N(0,1)$ We will compute the Markov, Chebyshev and Chernoff bounds on $P(|Z|>3)$. First, we'll need $E(|Z|)$\\\\

$$E(|Z|) =\frac1{\sqrt{2\pi}} \infi |z|e^{-\frac{z^2}{2}} dz$$
The integrand is even, so:
$$\frac1{\sqrt{2\pi}} \infi |z|e^{-\frac{z^2}{2}} dz = \frac2{\sqrt{2\pi}} \int_0^\infty ze^{-\frac{z^2}{2}} dz$$
We make the substitutions $u=z^2/2$, $du=zdz$:
$$\frac2{\sqrt{2\pi}} \int_0^\infty ze^{-\frac{z^2}{2}} dz = \frac2{\sqrt{2\pi}} \int_0^\infty e^{-u} du = \left. \frac2{\sqrt{2\pi}} -e^{-u}\right\rvert_0^\infty =\sqrt{\frac{2}{\pi}} $$

Markov: \\
$$P(|Z|>3) \leq \frac{E(|Z|)}{3}= \frac{1}{3}\sqrt{\frac{2}{\pi}} \approx 0.27$$

Chebyshev: \\
$$P(|Z|>3) \leq \frac19 \approx 0.11$$

Chernoff: \\

Because the standard normal is symmetric about $0$, 
$$P(|Z|>3) = 2P(Z>3)$$
and Chernoff yields:
$$2P(Z>3) \leq \frac{2e^{\frac{t^2}2}}{e^{3t}} \;\;\;\;\; t>0$$
To get the tightest bound, we minimize the right hand side. This is a problem that is more easily worked after taking the log - so we use the fact that log is strictly increasing, which means if the minimum of some function $g(t)$ occurs at $t_0$, min of $\log(g(t)$ also occurs at $t_0$ and vice-versa.

$$\log\left(\frac{2e^{\frac{t^2}2}}{e^{3t}}\right) = \log(2) + \log(e^{\frac{t^2}{2}}) + \log(e^{-3t})$$
$$ = \log(2) + \frac{t^2}{2}\log(e) - 3t\log(e)$$
$$ = \log(2) + \frac{t^2}{2} -3t$$
Now we differentiate:
$$\frac{d}{dt}\left(\log(2) + \frac{t^2}{2} -3t\right)$$
$$= t-3$$
Setting this equal to zero, we find our minimum at $t=3$. Thus, the best bound is:
$$2e^{\frac92}e^{-9} = 2e^{-\frac92} \approx 0.022$$
This gives us the best bound so far, but it is quite a lot larger than $0.003$!
\end{example}
\end{document}