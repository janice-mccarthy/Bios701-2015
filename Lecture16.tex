
\documentclass[12pt]{article} % Use A4 paper with a 12pt font size - different paper sizes will require manual recalculation of page margins and border positions

% Generated with LaTeXDraw 2.0.8
% Mon Jun 17 19:00:40 EDT 2013
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[left=1.3cm,right=4.6cm,top=1.8cm,bottom=4.0cm,marginparwidth=3.4cm]{geometry} % Adjust page margins
\usepackage{amsmath} % Required for equation customization
\usepackage{amssymb} % Required to include mathematical symbols
\usepackage{xcolor} % Required to specify colors by name
\usepackage{amsthm}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds,trees}
\usepackage{wasysym}

\makeatletter
\newcommand{\mytag}[2]{%
  \text{#1}%
  \@bsphack
  \protected@write\@auxout{}%
         {\string\newlabel{#2}{{#1}{\thepage}}}%
  \@esphack
}
\makeatother

\setlength{\parindent}{0cm} % Remove paragraph indentation
\newcommand{\tab}{\hspace*{2em}} % Defines a new command for some horizontal space
%\newcommand{\choose}[2]{\left(\begin{matrix}
%{#1}\\{#2}
%\end{matrix}\right)}
\date{}
\title{Introduction to Probability Theory - Lecture 16}
%----------------------------------------------------------------------------------------

\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{exer}{Exercises}
\newtheorem{thm}{Therorem}
\begin{document}
\maketitle

\section{Summaries of Distributions and Moments}
At this point, we have seen several examples of common distributions, both in the discrete and continuous cases. Often, we wish to consider certain properties of a distribution - ones that characterize it to some point, but do not completely characterize the distribution. For example, we have already seen the mean and variance. Many (wildly different) distributions may have the same mean and variance. Still, these values tell us \emph{something} about the distribution. In particular, the mean gives us a measure of \emph{center} and the variance gives us a measure of \emph{spread}.\\\\
\subsection{Additional Measures of Center}
In addition to the mean, there are a couple of other measures of the center (or the point that the probability is somehow 'balanced' around). \\

\begin{defn}
The \emph{median} of a  distribution its $50^{th}$ percentile, or equivalently: $m$ is the median of the distribution of a random variable $X$ $\iff$
$$P(X\leq m)\geq \frac12 \textrm{ and } P(X\geq m)\geq\frac12$$
\end{defn}
The second alternative definition is 'clunky', because we'd like to say the median $m$ is the point such that $F(m) = \frac12$, where $F$ is the CDF of $X$. But the CDF may not be continuous so that could give us trouble. We won't trouble ourselves too much with that - so the first definition is fine.\\\\
Another measure of center is a \emph{mode} of a distribution.\\
\begin{defn}
A \emph{mode} of a distribution is a value $c$ where its pmf/pdf is at a maximum. I.e.
$$\left\{c:f(c)\geq f(x) \textrm{ for all } x\right\}$$ 
\end{defn}
 Note that while the mean and median are unique values for a particular distribution, there can be many modes. \\\\
We won't use the mode for much in this class, so let's look at the mean versus the median in terms of which is a 'better' measure of center:\\\\
\begin{thm}
The value of $c$ that minimizes 
$$E((X-c)^2)$$
is $c=\mu$ where $\mu = E(X)$. The value of $c$ that minimizes
$$E(|X-c|)$$
is $c=m$ where $m$ is the median.
\end{thm}
There is a proof of this theorem in the book. Interested parties should read it! The important point is that 'better' can depend on the situation - the mean minimizes the average squared error - the median minimizes the average absolute error.\\\\
\paragraph{Estimators vs. Parameters/Properties of Distributions}
There is a fundamental and important difference between the \emph{population} values of the mean, median, mode, variance, etc. and the mean, median, mode, variance, etc. of a \emph{sample}. What we have defined above are properties of the underlying 'true' distribution of a population. \\\\
For example, suppose we want to know the average height of undergraduate students at Duke. We might randomly select (perhaps using records from the registrar) $100$ students and contact them and ask for their height. We take the average. That is the \emph{sample} mean. If we randomly select another $100$ students and take the mean, we would almost surely get a different value. In fact, \emph{the sample mean is a random variable!} \\\\
On the other hand, there is a 'true' mean, which we could obtain by taking the heights of \emph{all} Duke undergrads and averaging them. This value would not change if we repeated the process - in other words, \emph{the population mean is NOT random}.\\\\
Now, of course the sample and population values are related: in statistics we \emph{estimate} the population values from a sample and in many cases, we may make inference about how close we believe those sample values are to the true values. The question of which measures are 'best' in this sense will be addressed in 704.
\subsection{Moments}
As pointed out above, the summary measures we have defined may be the same for some \emph{very} different distributions. We can actually find a collection of summaries of a distribution, and if we consider all of them (an infinite number), they completely determine the distribution. These are called 'moments':\\\\
\begin{defn}
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. 
\begin{enumerate}
\item The $n^{th}$ moment (about zero) of $X$ is given by $E(X^n)$.
\item The $n^{th}$ central moment (or moment about the mean) is $E((X-\mu)^n)$
\item The $n^{th}$ standardized moment is given by $E\left(\left(\frac{X-\mu}{\sigma}\right)^n\right)$
\end{enumerate}
\end{defn} 

Note that the first moment about zero is the mean and the second moment about the mean is the variance. The third and fourth moments also have special names and interpretations.\\\\
\begin{defn}
The third standardized moment 
$$E\left(\left(\frac{X-\mu}{\sigma}\right)^3\right)$$
is called the \emph{skew} of a distribution.
\end{defn}
Of course, skew is related to symmetry:
\begin{defn}
A random variable has a distribution that is symmetric about its mean $\mu$ $\iff$
$$X-\mu \textrm{ and } \mu-X$$
have the same distribution. This is equivalent to 
$$f(\mu-x) = f(\mu+x)$$
where $f$ is the pdf of $X$.
\end{defn}
A distribution that is not symmetric is skew.\\\\
Note: If a distribution is symmetric about some value $\mu$, that value must be $E(X)$, if it exists. ($\mu$ is also the median in this case.)
\paragraph{Aside About Tails and Existence of Moments}
It is important to note that a valid pdf can have an infinite expectation or variance. For example, consider:
$$f(x) =\left\{\begin{matrix}\frac{1}{x^2} & x>1\\0&\textrm{otherwise}\end{matrix}\right.$$
We can easily see that this is a valid pdf. Clearly, it is strictly positive. 
Also,
$$\int_1^\infty \frac{1}{x^2}dx = \left.\frac{-1}{x}\right\rvert_1^\infty = 1$$
The CDF is
$$\int_1^x \frac1{t^2}dt = 1-\frac1{x} x>1$$
and is valid and continuous. The expectation:
$$E(X) = \int_1^\infty x \frac1{x^2}dx = \int_1^\infty \frac1{x}dx$$
which diverges. The variance and all higher moments also diverge, by similar computation. We say that such a distribution has 'fat tails'. In other words, the distribution doesn't decay very rapidly at the tails. Rapidly decaying tails result in the existence of higher moments.
\paragraph{End of Aside}


\begin{thm}
If a random variable $X$ has a symmetric distribution,
$$E(\left(X-\mu\right)^n) = 0 \textrm{ for } n \textrm{ odd}$$
\end{thm}

This theorem is proved in the text. A consequence of this theorem is that any odd moment detects skewness. Why do we use the third? One reason is that higher moments may not exist (if the tails of the distribution do not decay rapidly enough). As the tails of a distribution are important properties, there is a measure of how heavy or how long the tails of a distribution are:\\\\
\begin{defn}
The \emph{kurtosis} of a random variable $X$ is 
$$\mathrm{kurt}(X) = E\left(\left(\frac{X-\mu}{\sigma}\right)^4\right) - 3$$
\end{defn}
The '3' in the definition comes from the fourth moment of the standard normal - i.e. the kurtosis of a distribution is measured \emph{relative to the normal}. Large kurtosis means heavy tails, a high peak and 'low shoulders'. \\\\
The next topic in the book is on sample moments. We will skip this for now, and move on to how we can more easily calculate the moments of a distribution.
\subsection{Moment Generating Functions} 
\begin{defn}
The moment generating function (MGF) of a random variable $X$ is
$$M_X(t) = E(e^{tX})$$
if this is finite on an open interval $(-a,a)$ (i.e. an interval containing zero), otherwise the MGF does not exist. 
\end{defn}

Note: $M_X(0) = 1$, for any valid MGF. (Why?)\\

The following theorem gives us a nice way of computing moments:\\

\begin{thm}
The $n^{th}$ moment about zero of a random variable $X$ is given by:
$$E(X^n) = M_X^{(n)}(0)$$
where $M_X^{(n)}(0)$ is the $n^{th}$ derivative of $M_X(t)$ evaluated at $t=0$. 
\end{thm}

\begin{proof}
Recall that the Taylor expansion of a function $f(x)$ about $x=x_0$ is given by:
$$f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} \left(x-x_0\right)^n$$
If we expand about $x_0 = 0$, we obtain:
$$f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n$$
So, if we expand $M_X(t)$ about $t=0$, we have:
\begin{equation}\label{MGFseries} M_X(t) = \sum_{n=0}^\infty M_X^{(n)}(0) \frac{t^n}{n!} \end{equation}
Now, going back to the original definition of $M_X(t)$, we have:
$$M_X(t) = E(e^{tX})$$
But we can also write $e^{tX}$ as a series, by noting:
$$e^y = \sum_{n=0}^\infty \frac{y^n}{n!}$$
and substituting $y=tX$. We obtain:
$$M_X(t) = E(e^{tX}) = E\left(\sum_{n=0}^\infty \frac{(tX)^n}{n!}\right) = E\left(\sum_{n=0}^\infty \frac{X^nt^n}{n!}\right) = \sum_{n=0}^\infty E\left(\frac{X^nt^n}{n!}\right) =  \sum_{n=0}^\infty E\left(X^n\right)\frac{t^n}{n!}$$
Setting the above result equal to \eqref{MGFseries}, we see:
$$\sum_{n=0}^\infty M_X^{(n)}(0) \frac{t^n}{n!} = \sum_{n=0}^\infty E\left(X^n\right)\frac{t^n}{n!}$$
Now, if we equate the coefficients of $t^n$, we obtain:
$$M_X^{(n)}(0) = E(X^n)$$
\end{proof}

This is an important theorem, because it means we can find moments using \emph{differentiation}, rather than integration. Here is a second theorem that makes our lives easier!

\begin{thm}
If $X$ and $Y$ are independent, then:
$$M_{X+Y}(t) = M_X(t)M_Y(t)$$
\end{thm}
\begin{proof}
This is a simple consequence of a property of the exponential function and the expecation of a product of independent variables:
$$M_{X+Y}(t) = E\left(e^{t(X+Y)}\right) = E\left(e^{tX}e^{tY}\right)$$
Because $X$ and $Y$ are independent:
$$E\left(e^{tX}e^{tY}\right) = E\left(e^{tX}\right)E\left(e^{tY}\right) = M_X(t)M_Y(t)$$
\end{proof}
\end{document}