
\documentclass[12pt]{article} % Use A4 paper with a 12pt font size - different paper sizes will require manual recalculation of page margins and border positions

% Generated with LaTeXDraw 2.0.8
% Mon Jun 17 19:00:40 EDT 2013
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[left=1.3cm,right=4.6cm,top=1.8cm,bottom=4.0cm,marginparwidth=3.4cm]{geometry} % Adjust page margins
\usepackage{amsmath} % Required for equation customization
\usepackage{amssymb} % Required to include mathematical symbols
\usepackage{xcolor} % Required to specify colors by name
\usepackage{amsthm}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{shapes,backgrounds,trees}
\usepackage{wasysym}

\makeatletter
\newcommand{\mytag}[2]{%
  \text{#1}%
  \@bsphack
  \protected@write\@auxout{}%
         {\string\newlabel{#2}{{#1}{\thepage}}}%
  \@esphack
}
\makeatother

\setlength{\parindent}{0cm} % Remove paragraph indentation
\newcommand{\tab}{\hspace*{2em}} % Defines a new command for some horizontal space
%\newcommand{\choose}[2]{\left(\begin{matrix}
%{#1}\\{#2}
%\end{matrix}\right)}
\date{}
\title{Introduction to Probability Theory - Lecture 20}
%----------------------------------------------------------------------------------------

\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{exer}{Exercises}
\newtheorem{thm}{Therorem}
\begin{document}
\maketitle

\section{Joint Distributions - Continued}
Recall from last time, we defined the CDF, joint, marginal and conditional pmfs for jointly distributed discrete random varibles:
\begin{itemize}
\item joint pmf: $$f_{XY}(x,y) = P(X=x,Y=y)$$
\item joint CDF: $$F_{XY}(x,y) = P(X\leq x,Y\leq y) = \sum_{s\leq x}\sum_{t\leq y} f_{XY}(s,t)$$
\item marginal pmf of $X$:
$$f_X(x) = \sum_y f_{XY}(x,y)$$
and similarly, the marginal pmf of $Y$ is
$$f_{Y}(y) = \sum_x f_{XY}(x,y)$$  
\end{itemize}
And we wrote downa version of Bayes Rule and LOTP in terms of conditional and marginal pmfs. Bayes rule is:
$$f_{Y|X}(y) = \frac{f_{X|Y}(x) f_Y(y)}{f_X(x)}$$
and the LOTP is:
$$f_X(x) = \sum_y f_{X|Y})x) f_Y(y)$$
\subsection{Independence}
\begin{defn}
Two random variables $X$ and $Y$ are independent $\iff$ 
$$F_{XY}(x,y) = F_x(x)F_Y(y)$$
where $F_X(x)$ and $F_Y(y)$ are the marginal CDFs for $X$ and $Y$. An equivalent statement is that the joint pmf/pdf factors into the product of the marginals:
$$f_{XY}(x,y) = f_X(x)f_Y(y)$$
\end{defn} 
Note: we will see in some examples that it is not enough for $f_{XY}(x,y)$ to factor into two functions, one a function of $x$ and the other a function of $y$. This is because the variables may have a dependence based on the \emph{support} of the pmf/pdf.
\subsection{Examples of Discrete Joint Distributions}
We will consider two examples of multivariate discrete distributions: The exended hypergeometric distribution and the multinomial. As you may guess from the names, these are generalizations of the hypergeometric and binomial distributions.
\subsubsection{Extended Hypergeometric Distribution}
Suppose we have a finite collection of $N$ items, of $k+1$ different types. Let $M_1$ be the number of items of type 1, $M_2$ be the number of items of type 2, and so on up to $M_{k+1}$. We select $n$ items without replacement. Let $X=(X_1,...,X_k)$ be a random vector, where each $X_i$ is the number of items of type $i$ in the selection. The pmf of $X$ is given by:
$$f(x_1,...,x_k;n,M_1,...,M_k,N)=\left\{\begin{matrix}\frac{{M_1\choose{x_1}}{M_2\choose{x_2}}\cdots {M_k\choose{x_k}}{M_{k+1}\choose{x_{k+1}}}}{{N\choose{n}}}& 0\leq x_i\leq M_i\\\\\\ 0&\textrm{otherwise}\end{matrix}\right.$$
Note that there are really only $k$ variables, as $x_{k+1}$ is completely determined by the number chosen and the number of all the other $x_i$, and $M_{k+1}$ is completely determined by the other $M_i$ and $N$. I.e.:
$$x_{k+1} = n-\sum_{i=1}^k x_i \;\;\;\;\;\textrm{ and } \;\;\;\;\; M_{k+1} = N- \sum_{i=1}^k M_i$$
\begin{example}
Consider and urn with $4$ red marbles, $6$ blue marbles, $3$ green marbles and $4$ black marbles. There are $17$ marbles in total. Draw 10 marbles without replacement. Let $X=(X_1,X_2,X_3)$ where $X_1$ is the number of red marbles drawn, $X_2$ is the number of  blue marbles drawn and $X_3$ is the number of green marbles drawn (the number of black marbles drawn is $17-X_1-X_2-X_3$. We have, $M_1=4,M_2=6,M_3=3$ and $M_4 =4$. The probability that the sample drawn has $2$ red marbles, $5$ blue marbles, $2$ green marbles and $1$ black marble is:
$$f(2,5,2;10,4,6,3,17) =  \frac{{4\choose{2}}{6\choose{5}}{3\choose{2}}}{{17\choose{10}}}$$
\end{example}
\subsubsection{Multinomial Distribution}
Consider $k+1$ mutually exclusive, exhaustive events $E_1,...,E_k,E_{k+1}$. These events can be the outcome of any trial of a certain experiment. Suppose we perform $n$ independent trials of the experiment. Let $X_i$ be the number of occurrences of event $E_i$. (In the special case of $k+1=2$, there are two possible outcomes, and the trials are therefore Bernoulli). \\\\
The random vector $X=(X_1,...,X_{k+1})$ has the multinomial distribution with pmf:
$$f(x_1,...,x_k;n,p_1,...,p_k) = \left\{\begin{matrix}\frac{n!}{x_1!x_2!\cdots x_{k+1}!}p_1^{x_1}p_2^{x_2}\cdots p_{k+1}^{x_{k+1}}& 0\leq x_i\leq n\\\\\\ 0&\textrm{otherwise}\end{matrix}\right.$$
Where $p_i$ is the probability that event $E_i$ occurs. Once again, note that $x_{k+1}$ is completely determined by $x_1,...,x_k$ and the number of trials $n$, and $p_{k+1}$ is determined by $p_1,...,p_k$. I.e.:
$$x_{k+1} = n-\sum_{i=1}^k x_i \;\;\;\;\; \textrm{ and } \;\;\;\;\; p_{k+1} = 1-\sum_{i=1}^k p_i$$

Recall that a binomial random variable may be used to approximate a hypergeometric random variable (Because taking a relatively small sample from a large population without replacement is similar to taking the sample with replacement). Similarly, a multinomial random variable may be used to approximate an extended hypergeometric random variable.\\\\
\begin{example}
A 4-sided die is rolled 20 times. Consider the random vector $X=(X_1,X_2,X_3,X_4)$, where $X_i = $ number of times an $i$ is rolled. The probability of obtaining four 1's, six 2's, five 3's and 5 4's in the 20 rolls is:
$$f(4,6,5,5) = \frac{20!}{4!6!5!5!} (0.25)^{20}$$
\end{example} 

\paragraph{Marginal pmf}
Consider the random vector $X=(X_1,X_2)\sim Mult(n,p_1,p_2)$. What is the marginal pmf of $X_1$? We need to sum the joint pmf over all possible values of $x_2$:
$$f_1(x_1) = \sum_{x_2} f(x_1,x_2) = \sum_{x_2=0}^{n-x_1} f(x_1,x_2)$$
$$= \sum_{x_2=0}^{n-X_1} \frac{n!}{x_1!x_2!(n-x_1-x_2)!} p_1^{x_1}p_2^{x_2} \left(1-p_1-p_2\right)^{n-x_1-x_2}$$
Now, here is the part of this derivation that seems mysterious - but it is less so if we think for a moment: what \emph{might} the marginal pmf be? We might suspect it to be $Bin(n,p_1)$. At least that seems a reasonable guess. So we proceed with the derivation and see if we can coax the expression into the form:
$$\frac{n!}{x_1!(n-x_1)!}p_1^{x_1}(1-p_1)^{n-x_1}$$

So, we take our expression for the marginal pmf and multiply by $(n-x_)!/(n-x_1)!$ and factor and rearrage terms to obtain:
$$f_1(x_1) = \frac{n!}{x_1!(n-x_1)!}p_1^{x_1} \sum_{x_2=0}^{n-x_1} {{n-x_1}\choose{x_2}}p_2^{x_2}\left[1-p_1-p_2\right]^{n-x_1-x_2}$$
Now, we'll add one set of parenthesis:

$$f_1(x_1) = \frac{n!}{x_1!(n-x_1)!}p_1^{x_1} \sum_{x_2=0}^{n-x_1} {{n-x_1}\choose{x_2}}p_2^{x_2}\left[1-p_1-p_2\right]^{(n-x_1)-x_2}$$

If we squint, we can recognize the sum as a binomial expansion! In other words:

$$\sum_{x_2=0}^{n-x_1} {{n-x_1}\choose{x_2}}p_2^{x_2}\left[1-p_1-p_2\right]^{(n-x_1)-x_2} = \left(p_2 + \left[1-p_1-p_2\right]\right)^{n-x_1}$$
so that 
$$f_1(x_1) = \frac{n!}{x_1!(n-x_1)!}p_1^{x_1} \left(p_2 + \left[1-p_1-p_2\right]\right)^{n-x_1} = \frac{n!}{x_1!(n-x_1)!}p_1^{x_1} (1-p_1)^{n-x_1}$$
and we see that the marginal distribution of $X_1$ is $Bin(n,p_1)$, as we suspected. Note that without 'guessing', this derivation would be rather difficult!\\\\
We'll skip the 'Chicken-Egg' treatment in the book, and move directly on to the study of continuous joint distributions.

\subsection{Continuous Joint Distributions}
Note that the CDF has the same definition for both discrete and continuous random variables:
$$F(x,y) = P(X\leq x,Y\leq y)$$
Recall that in the single-variable case, given the CDF $F(x)$ for a random variable $X$, we define the pdf $f(x)$ to be: 
$$f(x) = \frac{d}{dx} F(x)$$
In the two variable case:
\begin{defn}
The joint pdf of two continuous random variables $X$ and $Y$ with joint CDF $F(x,y)$ is given by:
$$f_{XY}(x,y) = \frac{\partial^2}{\partial x\partial y}F_{XY}(x,y)$$
\end{defn}
Of course, joint pdfs must be valid:
\begin{enumerate}
\item $$f_{XY}(x,y) \geq 0$$
\item $$\int_{-\infty}^\infty\int_{-\infty}^\infty f_{XY}(x,y) dx dy = 1$$
\end{enumerate}
We obtain the probability that a two dimensional random vector lies in some region of the plane denoted $A$, we must integrate the pdf:
$$\int\int_A f_{XY}(x,y) dx dy$$
 
Just like the univariate case, the probability that $(X,Y) = (x,y)$ is exactly $0$. Additionally, the probabilty that a two-dimensional random vector $(X,Y)$ lies in a single line is also zero (because now the probability is a \emph{volume}, rather than an \emph{area}.)
\begin{defn}
Let $X$ and $Y$ be jointly distributed continuous random variables with joint pdf $F_{XY}(x,y)$. The marginal pdf of $X$ is:
$$f_X(x) = \int_{-\infty}^\infty f_{XY}(x,y) dy$$ 
\end{defn}
That is, we integrate the joint pdf over all values of $y$ to obtain the marginal pdf of $X$. We will see in several examples: \emph{we must be careful of supports}!\\\\
Just as in the discrete case, we can define conditional probability:
\begin{defn}
Given two jointly distributed continuous random variables $X$ and $Y$ with joint pdf $F_{XY}(x,y)$, we define the conditional pdf of $Y$ given $X=x$ to be:
$$f_{Y|x}(y) = \frac{f_{XY}(x,y)}{f_X(x)}$$
where $f_X(x)$ is the marginal pdf of $X$.
\end{defn}
Note: this is usually considered to be a function of $y$, for a fixed value of $x$.\\\\
We state without proof that Bayes Rule and LOTP work:
\begin{thm}
For jointly distributed continuous random variables $X$ and $Y$:
$$f_{Y|x}(y) = \frac{f_{X|y}(x)f_Y(y)}{f_X(x)}$$
and
$$f_X(x) = \int_{-\infty}^\infty f_{X|y}(x) f_Y(y)dy$$
\end{thm}
Just as we have done for jointly distributed discrete random variables, we now have defined the joint pdf, CDF, marginal and conditional distributions for jointly distributed continuous random variables and have seen the analogous form of Bayes Rule and LOTP.
\end{document}